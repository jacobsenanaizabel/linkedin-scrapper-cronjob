name: LinkedIn Scraper Advanced (RIGGeqD6RqKmlVoQU)

on:
  schedule:
    - cron: '0 9 */4 * *'  # A cada 3 dias Ã s 09:00 UTC
  workflow_dispatch:

jobs:
  scrape-and-download:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install requests pandas

      - name: Run Scraper and Download Results
        env:
          APIFY_TOKEN: ${{ secrets.APIFY_TOKEN }}
        run: |
          python3 << 'EOF'
          import requests
          import json
          import time
          import os
          from datetime import datetime
          import pandas as pd

          API_TOKEN = os.environ['APIFY_TOKEN']
          ACTOR_ID = "RIGGeqD6RqKmlVoQU"

          # ===================================================================
          # CONFIGURAÃ‡ÃƒO DAS BUSCAS
          # ===================================================================
          
          searches = [
              {
                  "name": "Senior Frontend Engineer",
                  "payload": {
                      "title": "Senior Frontend Engineer",
                      "location": "spain",
                      "limit": 50,
                      "contractType": ["F"],
                      "experienceLevel": [4"],
                      "datePosted": "r604800"
                  }
              },
              {
                  "name": "Desarrollador React",
                  "payload": {
                      "title": "Desarrollador React",
                      "location": "spain",
                      "limit": 50,
                      "contractType": ["F"],
                      "experienceLevel": [4"],
                      "datePosted": "r604800"
                  }
              },
              {
                  "name": "Staff Engineer",
                  "payload": {
                      "title": "Staff Engineer",
                      "location": "spain",
                      "limit": 50,
                      "contractType": ["F"],
                      "experienceLevel": ["4", "5"],
                      "datePosted": "r604800"
                  }
              },
              {
                  "name": "Frontend Lead",
                  "payload": {
                      "title": "Frontend Lead",
                      "location": "spain",
                      "limit": 50,
                      "contractType": ["F"],
                      "experienceLevel": ["4", "5"],
                      "datePosted": "r604800"
                  }
              },
              {
                  "name": "Fullstack Software Engineer",
                  "payload": {
                      "title": "Fullstack Software Engineer ",
                      "location": "spain",
                      "limit": 50,
                      "contractType": ["F"],
                      "experienceLevel": ["4"],
                      "datePosted": "r604800"
                  }
              }
          ]

          # ===================================================================
          # STEP 1: INICIAR TODOS OS SCRAPERS
          # ===================================================================
          
          print("=" * 70)
          print(f"ðŸš€ STEP 1: Starting {len(searches)} LinkedIn Scrapers")
          print("=" * 70)
          
          url = f"https://api.apify.com/v2/acts/{ACTOR_ID}/runs"
          headers = {"Authorization": f"Bearer {API_TOKEN}", "Content-Type": "application/json"}
          
          runs = []
          
          for search in searches:
              print(f"\nðŸ“Š Starting: {search['name']}")
              print(f"   Title: {search['payload']['title']}")
              print(f"   Location: {search['payload']['location']}")
              print(f"   Limit: {search['payload']['limit']}")
              
              response = requests.post(url, json=search['payload'], headers=headers)
              
              if response.status_code == 201:
                  run_id = response.json()['data']['id']
                  runs.append({
                      "name": search['name'],
                      "run_id": run_id,
                      "payload": search['payload']
                  })
                  print(f"   âœ… Started! Run ID: {run_id}")
              else:
                  print(f"   âŒ Failed: {response.status_code}")
                  print(f"   {response.text}")
          
          if len(runs) == 0:
              print("\nâŒ No scrapers started successfully!")
              exit(1)
          
          print(f"\nâœ… Successfully started {len(runs)}/{len(searches)} scrapers")

          # ===================================================================
          # STEP 2: AGUARDAR CONCLUSÃƒO DE TODOS OS RUNS
          # ===================================================================
          
          print("\n" + "=" * 70)
          print(f"â±ï¸  STEP 2: Waiting for completion (max 10 min each)")
          print("=" * 70)
          
          max_wait = 600  # 10 minutos por run
          completed_runs = []
          
          for run in runs:
              run_id = run['run_id']
              name = run['name']
              
              print(f"\nâ³ Waiting for: {name}")
              print(f"   Run ID: {run_id}")
              
              start_time = time.time()
              
              while time.time() - start_time < max_wait:
                  # Endpoint CORRETO: /v2/actor-runs/
                  status_url = f"https://api.apify.com/v2/actor-runs/{run_id}"
                  status_response = requests.get(status_url, headers={"Authorization": f"Bearer {API_TOKEN}"})
                  
                  if status_response.status_code == 200:
                      run_data = status_response.json()['data']
                      status = run_data['status']
                      
                      if status == "SUCCEEDED":
                          elapsed = int(time.time() - start_time)
                          # Guardar dataset ID para download
                          run['dataset_id'] = run_data.get('defaultDatasetId')
                          print(f"   âœ… Completed in {elapsed}s")
                          print(f"   ðŸ“Š Dataset ID: {run['dataset_id']}")
                          completed_runs.append(run)
                          break
                      elif status in ["FAILED", "ABORTED", "TIMED-OUT"]:
                          print(f"   âŒ Failed with status: {status}")
                          break
                      elif status == "RUNNING":
                          # Atualiza a cada 15 segundos
                          time.sleep(15)
                      else:
                          # Para outros status, aguarda mais
                          time.sleep(10)
                  else:
                      print(f"   âš ï¸ Error checking status: {status_response.status_code}")
                      break
              
              if time.time() - start_time >= max_wait:
                  print(f"   âš ï¸ Timeout after 10 minutes")

          print(f"\nâœ… Completed: {len(completed_runs)}/{len(runs)} runs")

          # ===================================================================
          # STEP 3: DOWNLOAD DE TODOS OS RESULTADOS
          # ===================================================================
          
          print("\n" + "=" * 70)
          print(f"ðŸ“¥ STEP 3: Downloading results")
          print("=" * 70)
          
          all_jobs = []
          stats = {
              "total_jobs": 0,
              "by_search": {}
          }
          
          for run in completed_runs:
              run_id = run['run_id']
              dataset_id = run.get('dataset_id')
              name = run['name']
              
              print(f"\nðŸ“¥ Downloading: {name}")
              print(f"   Run ID: {run_id}")
              print(f"   Dataset ID: {dataset_id}")
              
              if not dataset_id:
                  print(f"   âš ï¸ No dataset ID found, trying alternative method...")
                  dataset_url = f"https://api.apify.com/v2/actor-runs/{run_id}/dataset/items"
              else:
                  # Endpoint CORRETO: /v2/datasets/{dataset_id}/items
                  dataset_url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
              
              print(f"   ðŸ”— URL: {dataset_url}")
              
              results_response = requests.get(
                  dataset_url,
                  headers={"Authorization": f"Bearer {API_TOKEN}"},
                  params={"clean": "true", "format": "json"}
              )
              
              print(f"   ðŸ“¡ Status Code: {results_response.status_code}")
              
              if results_response.status_code == 200:
                  jobs = results_response.json()
                  
                  for job in jobs:
                      job['_search_name'] = name
                      job['_search_title'] = run['payload']['title']
                      job['_search_location'] = run['payload']['location']
                  
                  all_jobs.extend(jobs)
                  
                  stats['by_search'][name] = len(jobs)
                  stats['total_jobs'] += len(jobs)
                  
                  print(f"   âœ… Downloaded {len(jobs)} jobs")
              else:
                  print(f"   âŒ Failed to download: {results_response.status_code}")
                  print(f"   Response: {results_response.text[:500]}")

          # ===================================================================
          # STEP 4: SALVAR RESULTADOS
          # ===================================================================
          
          print("\n" + "=" * 70)
          print(f"ðŸ’¾ STEP 4: Saving results")
          print("=" * 70)
          
          if all_jobs:
              # Salvar JSON completo
              with open('jobs_latest.json', 'w', encoding='utf-8') as f:
                  json.dump(all_jobs, f, ensure_ascii=False, indent=2)
              print(f"âœ… Saved jobs_latest.json ({len(all_jobs)} jobs)")
              
              # Criar CSV simplificado
              try:
                  df = pd.DataFrame(all_jobs)
                  
                  # Selecionar colunas mais importantes (ajusta conforme o output real do actor)
                  # Como nÃ£o sabemos exatamente quais campos o actor retorna, vamos tentar campos comuns
                  possible_columns = [
                      'title', 'company', 'location', 'url', 'link', 'jobUrl',
                      'description', 'postedAt', 'salary', 'employmentType',
                      '_search_name', '_search_title', '_search_location'
                  ]
                  
                  export_cols = [col for col in possible_columns if col in df.columns]
                  
                  if export_cols:
                      df[export_cols].to_csv('jobs_latest.csv', index=False, encoding='utf-8')
                      print(f"âœ… Saved jobs_latest.csv ({len(export_cols)} columns)")
                  else:
                      # Se nÃ£o encontrar colunas conhecidas, exporta todas
                      df.to_csv('jobs_latest.csv', index=False, encoding='utf-8')
                      print(f"âœ… Saved jobs_latest.csv (all columns)")
              
              except Exception as e:
                  print(f"âš ï¸ Could not create CSV: {e}")
              
              # Salvar estatÃ­sticas
              stats['timestamp'] = datetime.now().isoformat()
              stats['searches_executed'] = len(completed_runs)
              stats['searches_total'] = len(searches)
              
              with open('summary.json', 'w', encoding='utf-8') as f:
                  json.dump(stats, f, indent=2)
              print(f"âœ… Saved summary.json")
              
              # Salvar info dos runs
              run_info = []
              for run in completed_runs:
                  run_info.append({
                      "name": run['name'],
                      "run_id": run['run_id'],
                      "url": f"https://console.apify.com/actors/runs/{run['run_id']}"
                  })
              
              with open('run_info.json', 'w', encoding='utf-8') as f:
                  json.dump(run_info, f, indent=2)
              print(f"âœ… Saved run_info.json")
              
          else:
              print("âš ï¸ No jobs to save")
              stats['total_jobs'] = 0
              stats['timestamp'] = datetime.now().isoformat()
              with open('summary.json', 'w', encoding='utf-8') as f:
                  json.dump(stats, f, indent=2)

          # ===================================================================
          # STEP 5: RESUMO FINAL
          # ===================================================================
          
          print("\n" + "=" * 70)
          print("ðŸ“Š FINAL SUMMARY")
          print("=" * 70)
          print(f"Total jobs found: {stats['total_jobs']}")
          print(f"Searches completed: {len(completed_runs)}/{len(searches)}")
          print("\nJobs by search:")
          for search_name, count in stats['by_search'].items():
              print(f"  â€¢ {search_name}: {count} jobs")
          print("\nâœ… Workflow completed successfully!")
          print("=" * 70)
          EOF

      - name: Upload Results
        uses: actions/upload-artifact@v4
        with:
          name: linkedin-jobs-${{ github.run_number }}
          path: |
            jobs_latest.json
            jobs_latest.csv
            summary.json
            run_info.json
          retention-days: 90

      - name: Create Summary
        run: |
          echo "## ðŸŽ¯ LinkedIn Job Scraper - Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f summary.json ]; then
            TOTAL=$(jq -r '.total_jobs' summary.json)
            SEARCHES=$(jq -r '.searches_executed' summary.json)
            
            echo "**Total Jobs Found:** $TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "**Searches Completed:** $SEARCHES" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            echo "### ðŸ“Š Jobs by Search" >> $GITHUB_STEP_SUMMARY
            jq -r '.by_search | to_entries[] | "- **\(.key):** \(.value) jobs"' summary.json >> $GITHUB_STEP_SUMMARY
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“¥ Download Results" >> $GITHUB_STEP_SUMMARY
            echo "Scroll down to **Artifacts** section to download:" >> $GITHUB_STEP_SUMMARY
            echo "- \`jobs_latest.json\` - Full dataset" >> $GITHUB_STEP_SUMMARY
            echo "- \`jobs_latest.csv\` - Spreadsheet format" >> $GITHUB_STEP_SUMMARY
            echo "- \`summary.json\` - Statistics" >> $GITHUB_STEP_SUMMARY
            echo "- \`run_info.json\` - Run IDs and links" >> $GITHUB_STEP_SUMMARY
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ”— View in Apify Console" >> $GITHUB_STEP_SUMMARY
            jq -r '.[] | "- [\(.name)](\(.url))"' run_info.json >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "Run info not available" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Summary not available" >> $GITHUB_STEP_SUMMARY
          fi